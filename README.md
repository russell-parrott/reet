[![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)

# REET - the Structural Governance Standard for AI

This repository is the authoritative public record of REET — the Structural Governance Standard for AI.

REET is a complete, verifiable oversight framework built on the Q1–Q15 verification set. It includes structural questions, pass/fail criteria, and evidence requirements.

The Standard defines AI governance as a discipline in its own right. Compliance is treated not as a declaration of intent but as a system condition that can be independently verified in operation.

It establishes a universal breach taxonomy, operational countermeasures, structural verification methods, and dynamic safeguards to ensure that governance remains enforceable over time.

The Standard sets the conditions under which trust in AI systems can be established, preserved, and proven.

## Components

| Directory | Description |
|-----------|-------------|
| `/tests/` | Structural test definitions linked to each question and safeguard. |
| `/policy/` | Cross-jurisdiction governance notes and evidence chain considerations. |
| `./` | Publication record and provenance documentation. |

## Design Principles

REET is based on the principle that trust in AI cannot be declared — it must be a system condition that is designed and able to be proven in operation.

Each safeguard is mapped to a structural question, with defined criteria for verifying compliance, and anchored to an audit trail capable of withstanding cross-jurisdictional challenges.

## Provenance

REET - the Structural Governance Standard for AI was originated and published by Russell Parrott in 2025.

This repository constitutes the authoritative public record of the doctrine, including its definitions, criteria, and canonical schemas.

DOI: [![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)


---

**Author:** Russell Parrott — Author, *Trust in Systems*  
**License:** CC BY-ND 4.0 (Attribution required, no derivatives)  
**Repository Description:** Safeguard to halt AI before harm.  
