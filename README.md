[![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)

# REET - the Structural Governance Standard for AI

This repository is the authoritative public record of REET, the Structural Governance Standard for AI.

REET is a complete and verifiable oversight framework. It is defined through fifteen structural tests (Q1–Q15), each with specific questions, pass/fail criteria, and evidence formats. Together, these tests provide a binary outcome: either the safeguards hold, or accountability fails.

The Standard establishes AI governance as a discipline in its own right. Compliance is not treated as a statement of policy or intent but as a structural condition engineered into the system and independently verifiable in operation.

REET integrates:

- A universal taxonomy of structural breaches,
- Operational countermeasures to prevent recurrence,
- Methods of verification that withstand independent audit, and
- Safeguards designed to adapt under changing conditions.

Through these elements, REET sets the enforceable conditions under which trust in AI systems can be established, preserved, and demonstrably proven.

## Components

| Directory | Description |
|-----------|-------------|
| `/tests/` | Structural test definitions linked to each question and safeguard. |
| `/policy/` | Cross-jurisdiction governance notes and evidence chain considerations. |
| `./SPONSORSHIPmd` | Sponsorship arrangements and conditions for sustaining REET as a public standard. |
| `./` | Publication record and provenance documentation. |

## Design Principles

REET is based on the principle that trust in AI cannot be declared — it must be a system condition that is designed and able to be proven in operation.

Each safeguard is mapped to a structural question, with defined criteria for verifying compliance, and anchored to an audit trail capable of withstanding cross-jurisdictional challenges.

## Provenance

REET - the Structural Governance Standard for AI was originated and published by Russell Parrott in 2025.

This repository constitutes the authoritative public record of the doctrine, including its definitions, criteria, and canonical schemas.

DOI: [![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)


---

**Author:** Russell Parrott — Author, *Trust in Systems*  
**License:** CC BY-ND 4.0 (Attribution required, no derivatives)  
**Repository Description:** Safeguard to halt AI before harm.  
