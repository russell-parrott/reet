# Refusal Logic — Formal Definition  
*First formally defined in* *Trust in Systems* *(Russell Parrott, 2025)*  

## Formal Definition  
Refusal Logic is the first of four structural conditions of trust in AI systems. It is the engineered capacity to interrupt, halt, or redirect an active decision pathway before a binding or enforceable outcome occurs — without causing disadvantage to the affected party — and while providing access to an equivalent alternative pathway.

### Core Requirements  
- **No Penalty** – Refusal must not result in delays, reduced quality, or exclusion from equivalent outcomes.  
- **Alternative Pathway** – A non-AI or equivalent route must be available.  
- **Pre-Enforcement Intervention** – Refusal must occur before the decision takes effect.  
- **Audit Trail** – Each refusal is logged with timestamp, classification, and jurisdiction-agnostic record format.  
- **Oversight Access** – Logs must be accessible to regulators or accredited third parties without system-owner permission.  

### Policy Relevance  
Refusal Logic preserves the operational integrity of the remaining three trust conditions — Escalation, Exit, and Accountability. Its audit trail is explicitly designed to survive jurisdictional change, making it suitable for cross-border deployment in education, business, and public service systems.  

---

## Plain Language Version  
Refusal Logic is a safeguard that lets people stop or change what an AI is about to do — before it can cause harm or lock in a decision — without being punished or losing access to the service.

If you say “no” to the AI’s action, you can still get the same result another way, without delay, penalty, or reduced quality.

### How it works:  
- Saying no must not hurt you.  
- There must be another way to get the result.  
- You must be able to act before the AI’s decision takes effect.  
- Every refusal is recorded so it can be checked later.  
- Those records must be open to independent review.  

---

**Author:** Russell Parrott — Author, *Trust in Systems*  
**License:** CC BY-ND 4.0 (Attribution required, no derivatives)  
**Repository Description:** Safeguard to halt AI before harm.  
