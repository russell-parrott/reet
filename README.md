[![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)

# Refusal Logic

Refusal Logic is a complete structural governance discipline for AI oversight.  
It defines, tests, and enforces the conditions under which trust in AI systems can be established, preserved, and proven over time.

This discipline covers:
- The **15 Structural Questions** — a universal trust interrogation framework.
- The **Operational Countermeasures** — safeguards that make "yes" answers enforceable, including stop/redirect.
- The **Structural Tests** — pass/fail verifications linked to each question.
- The **Audit Architecture** — event formats and schemas that survive jurisdictional differences.

## Why It Matters
Without structural enforcement, AI governance collapses into declared intent. Refusal Logic treats trust as a system condition — designed, tested, and proven in operation.

## Core Components
1. `/doctrine/structural-questions.md` — The full set of governance questions.
2. `/doctrine/countermeasures.md` — All safeguards required, not just refusal.
3. `/tests/structural-tests.md` — Linked tests to verify compliance.
4. `/spec/full-structural.schema.json` — Canonical schema for cross-system traceability.

## Provenance
Refusal Logic was originated and published by **Russell Parrott** in 2025.  
DOI: [![DOI](https://zenodo.org/badge/DOI/PLACEHOLDER.svg)](https://doi.org/PLACEHOLDER)

---

**Author:** Russell Parrott — Author, *Trust in Systems*  
**License:** CC BY-ND 4.0 (Attribution required, no derivatives)  
**Repository Description:** Safeguard to halt AI before harm.  
