[![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)

# Refusal Logic

Refusal Logic is a complete structural governance discipline for AI oversight.  
It defines, tests, and enforces the conditions under which trust in AI systems can be established, preserved, and proven over time.

## Scope

Refusal Logic covers the full framework required to make AI governance enforceable in operation.  
This includes:

- **The 15 Structural Questions** — a universal trust interrogation framework applicable across jurisdictions and sectors.
- **Operational Countermeasures** — safeguards that make affirmative answers enforceable in live systems, including but not limited to the stop/redirect mechanism.
- **Structural Tests** — pass/fail verifications linked to each structural question to prove compliance.
- **Audit Architecture** — event formats and schemas designed to ensure evidence chains survive jurisdictional differences and organisational boundaries.

## Components

| Directory | Description |
|-----------|-------------|
| `/doctrine/` | Foundational documents, including the 15 Structural Questions and the full set of operational countermeasures. |
| `/tests/` | Structural test definitions linked to each question and safeguard. |
| `/spec/` | Canonical data schemas and export formats for refusal events, audit logs, and structural compliance reporting. |
| `/policy/` | Cross-jurisdiction governance notes and evidence chain considerations. |
| `/history/` | Publication record and provenance documentation. |

## Design Principles

Refusal Logic is based on the principle that trust in AI cannot be declared — it must be a system condition that is designed, tested, and proven in operation.  
Each safeguard is mapped to a structural question, verified through an operational test, and anchored to an audit trail capable of withstanding cross-jurisdictional challenges.

## Provenance

Refusal Logic was originated and published by **Russell Parrott** in 2025.  
This repository constitutes the authoritative public record of the doctrine, including its definitions, test methods, and canonical schemas.

DOI: [![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)


---

**Author:** Russell Parrott — Author, *Trust in Systems*  
**License:** CC BY-ND 4.0 (Attribution required, no derivatives)  
**Repository Description:** Safeguard to halt AI before harm.  
