[![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)

# Structural Governance Standard for AI

This repository is the authoritative public record of the Structural Governance Standard for AI — a complete, testable oversight framework built on the Q1–Q15 verification set, including procedures, pass/fail criteria, and evidence formats.

The Standard formally defines AI governance as a discipline in its own right. It treats compliance not as a statement of intent, but as an engineered system condition that is designed, tested, and proven in operation.

It integrates a universal breach taxonomy, operational countermeasures, live structural tests, and dynamic safeguards to ensure governance remains enforceable over time.

The Standard defines, tests, and enforces the conditions under which trust in AI systems can be established, preserved, and demonstrably proven.

## Components

| Directory | Description |
|-----------|-------------|
| `/doctrine/` | Foundational documents, including the 15 Structural Questions and the full set of operational countermeasures. |
| `/tests/` | Structural test definitions linked to each question and safeguard. |
| `/policy/` | Cross-jurisdiction governance notes and evidence chain considerations. |
| `./` | Publication record and provenance documentation. |

## Design Principles

Structural Governance Standard for AI is based on the principle that trust in AI cannot be declared — it must be a system condition that is designed, tested, and proven in operation.

Each safeguard is mapped to a structural question, verified through an operational test, and anchored to an audit trail capable of withstanding cross-jurisdictional challenges.

## Provenance

Structural Governance Standard for AI was originated and published by **Russell Parrott** in 2025.  

This repository constitutes the authoritative public record of the doctrine, including its definitions, test methods, and canonical schemas.

DOI: [![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)


---

**Author:** Russell Parrott — Author, *Trust in Systems*  
**License:** CC BY-ND 4.0 (Attribution required, no derivatives)  
**Repository Description:** Safeguard to halt AI before harm.  
