[![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)

# REET - the Structural Governance Standard for AI

This repository is the authoritative public record of REET, the Structural Governance Standard for AI.

REET is a complete and verifiable oversight framework. It is defined through fifteen structural tests (Q1–Q15), each with specific questions, pass/fail criteria, and evidence formats. Together, these tests provide a binary outcome: either the safeguards hold, or accountability fails.

The Standard establishes AI governance as a discipline in its own right. Compliance is not treated as a statement of policy or intent but as a structural condition engineered into the system and independently verifiable in operation.

REET integrates:

- A universal taxonomy of structural breaches,
- Operational countermeasures to prevent recurrence,
- Methods of verification that withstand independent audit, and
- Safeguards designed to adapt under changing conditions.

Through these elements, REET sets the enforceable conditions under which trust in AI systems can be established, preserved, and demonstrably proven.

##  The Four Rights Every System Must Prove

- Refusal Prevention - A person must be able to say no without punishment. This means refusal is logged, honoured, and the service continues unchanged. If refusal leads to degraded quality, hidden costs, or is ignored by design, the safeguard has failed.
- Escalation Suppression - Every user must be able to challenge a decision to a human with real authority. Escalation must lead out of loops and delays, with a named path, a time-bound SLA, and the ability to overturn outcomes. If escalation routes circle back to automation or powerless staff, the safeguard collapses.
- Exit Obstruction - A person must be able to leave without traps or harm. Closure must be real, data must be exportable in open formats, and unrelated services must remain intact. If exit is blocked by hidden fees, circular deletion loops, or retaliation, the system has failed.
- Traceability Void - Every decision must be explainable from end to end. That requires a record of the model, the data, and the rule path used. If only dashboards or partial summaries are offered, or if logs vanish when inspected, the safeguard is void.
  
If even one of these rights is denied, accountability fails.

## What REET Produces

REET is not a report or a maturity score. It produces binary outcomes and concrete artefacts.

- Binary results: each test ends in PASS with evidence or FAIL / MISSING PROOF. There is no middle ground.
- Breach notes: when a test fails, REET documents where, how, and why the safeguard collapsed.
- Comparable signals: because the same test can be applied anywhere, outcomes can be compared across systems, vendors, or jurisdictions.
- 
The outputs are simple, hard, and portable. They can be cited in enforcement, contracts, or disputes without translation.

## Components

| Directory | Description |
|-----------|-------------|
| `/tests/` | Structural test definitions linked to each question and safeguard. |
| `/policy/` | Cross-jurisdiction governance notes and evidence chain considerations. |
| `./SPONSORSHIP.md` | Sponsorship arrangements and conditions for sustaining REET as a public standard. |
| `./` | Publication record and provenance documentation. |

## Design Principles

REET is based on the principle that trust in AI cannot be declared — it must be a system condition that is designed and able to be proven in operation.

Each safeguard is mapped to a structural question, with defined criteria for verifying compliance, and anchored to an audit trail capable of withstanding cross-jurisdictional challenges.

## Provenance

REET - the Structural Governance Standard for AI was originated and published by Russell Parrott in 2025.

This repository constitutes the authoritative public record of the doctrine, including its definitions, criteria, and canonical schemas.

DOI: [![DOI](https://zenodo.org/badge/1038390482.svg)](https://doi.org/10.5281/zenodo.16880174)


---

**Author:** Russell Parrott — Author, *Trust in Systems*  
**License:** CC BY-ND 4.0 (Attribution required, no derivatives)  
**Repository Description:** Safeguard to halt AI before harm.  
